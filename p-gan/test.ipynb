{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nz = 100\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#wgan generator\n",
    "class wgan_generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(wgan_generator,self).__init__()\n",
    "        #input: [batch_size,100]\n",
    "        self.fc1 = nn.utils.weight_norm(nn.Linear(nz,2*4*4*1024))\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.utils.weight_norm(nn.Conv2d(1024,2*512,kernel_size=5,padding=2,stride=1))\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.utils.weight_norm(nn.Conv2d(512,2*256,kernel_size=5,padding=2,stride=1))\n",
    "        )\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.utils.weight_norm(nn.Conv2d(256,2*128,kernel_size=5,padding=2,stride=1))\n",
    "        )\n",
    "        \n",
    "        self.conv4 = nn.utils.weight_norm(nn.Conv2d(128,3,kernel_size=5,padding=2,stride=1))\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x,l = torch.chunk(x,2,dim=1)\n",
    "        x = x * F.sigmoid(l) # gated linear unit, one of Alec's tricks\n",
    "        \n",
    "        x = x.view(-1,1024,4,4)\n",
    "        #[4,4]\n",
    "        x = self.conv1(x)\n",
    "        x,l = torch.chunk(x,2,dim=1)\n",
    "        x = x * F.sigmoid(l)\n",
    "        #[8,8]\n",
    "        x = self.conv2(x)\n",
    "        x,l = torch.chunk(x,2,dim=1)\n",
    "        x = x * F.sigmoid(l)\n",
    "        #[16,16]\n",
    "        x = self.conv3(x)\n",
    "        x,l = torch.chunk(x,2,dim=1)\n",
    "        x = x * F.sigmoid(l)\n",
    "        #[32,32]\n",
    "        x = self.conv4(x)\n",
    "        \n",
    "        x = self.tanh(x)\n",
    "        return x;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试cuda代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    import numpy as np\n",
    "    import ctypes\n",
    "    ll = ctypes.cdll.LoadLibrary\n",
    "    lib = ll(\"./libhungarian.so\")\n",
    "    #lib.func.argtypes = [ctypes.c_float]\n",
    "    #set up function\n",
    "    c = np.abs(np.random.randn(600,600))\n",
    "    simplex_func = lib.func\n",
    "    simplex_func.restype = ctypes.POINTER(ctypes.c_int)\n",
    "    simplex_func.argtypes = [ctypes.POINTER(ctypes.c_float)]\n",
    "    #print simplex_func()\n",
    "    #print '***finish***'\n",
    "    #batch_size = 10\n",
    "    #build simplexTableau\n",
    "    c =  c.flatten().tolist()\n",
    "    c_value =(ctypes.c_float * len(c))(*c)\n",
    "    result = simplex_func(c_value)\n",
    "    for i in range(batch_size):\n",
    "    \tpi[result[i]] = 1.0/batch_size\n",
    "    pi = pi.reshape(batch_size,-1)\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "./libhungarian.so: undefined symbol: func",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-daede8cfdec4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#lib.func.argtypes = [ctypes.c_float]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#set up function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0msimplex_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/lrh/anaconda2/lib/python2.7/ctypes/__init__.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lrh/anaconda2/lib/python2.7/ctypes/__init__.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, name_or_ordinal)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_or_ordinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FuncPtr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_ordinal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_ordinal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname_or_ordinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: ./libhungarian.so: undefined symbol: func"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import ctypes\n",
    "ll = ctypes.cdll.LoadLibrary\n",
    "lib = ll(\"./libhungarian.so\")\n",
    "#lib.func.argtypes = [ctypes.c_float]\n",
    "#set up function\n",
    "simplex_func = lib.func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "c = np.abs(np.random.randn(600,600))\n",
    "print c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = torch.randn(4,5,80)\n",
    "\n",
    "b = torch.chunk(a,20,dim=2)\n",
    "\n",
    "print b[0].shape\n",
    "print a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dset\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "from scipy.optimize import linprog\n",
    "import torch.nn.functional as F\n",
    "import utils\n",
    "\n",
    "g_steps = 5\n",
    "result_directory = \"cifar_real_samples\"\n",
    "batch_size = 64\n",
    "bn = True\n",
    "lr = 3e-4\n",
    "beta = (0.5,0.999)\n",
    "epoch_num = 200\n",
    "cuda = True\n",
    "\n",
    "#Linear Program\n",
    "a1 = np.zeros([batch_size*batch_size,batch_size]);\n",
    "a2 = np.zeros([batch_size*batch_size,batch_size])\n",
    "for i in range(batch_size):\n",
    "    a1[batch_size*i:batch_size*(i+1),i] = 1\n",
    "    a2[batch_size*i:batch_size*(i+1),:] = np.eye(batch_size,batch_size)\n",
    "A = np.concatenate((a1,a2),axis=1)\n",
    "A = A.T\n",
    "b = np.ones([batch_size*2])/batch_size\n",
    "\n",
    "#wgan generator\n",
    "class wgan_generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(wgan_generator,self).__init__()\n",
    "        #input: [batch_size,100]\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "\n",
    "        return x;\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "def loss_fn(real_x,fake_x,isopt=True,pi=None):\n",
    "    batch_size = real_x.size(0)\n",
    "    cost_matrix = torch.zeros([batch_size,batch_size]).cuda()\n",
    "    dist_matrix = np.zeros([batch_size,batch_size])\n",
    "    for i in range(batch_size):\n",
    "        diff = real_x - fake_x[i]\n",
    "        diff = diff.view(diff.size(0),-1)\n",
    "        #||x_i - y_j||_2\n",
    "        #[batch_size,1]\n",
    "        diff_norm = diff.norm(2 , dim = 1)\n",
    "        #||x_i - y_j||_1\n",
    "        diff_norm_1 = diff.norm(1 , dim = 1)\n",
    "        #cosin(x_i,y_j)\n",
    "        \n",
    "        #sobolev spaces\n",
    "        \n",
    "        cost_matrix[:,i] = diff_norm + 0.05 * diff_norm_1\n",
    "        #min_value,min_pos = torch.min(diff_norm,dim=0)\n",
    "        #print min_pos\n",
    "        #dist_matrix[min_pos,i] = 1.0/batch_size\n",
    "    #[batch_size,batch_size]\n",
    "    if isopt:\n",
    "        c = cost_matrix.cpu().detach().numpy().reshape(batch_size*batch_size)\n",
    "        pi = utils.simplexCVX(batch_size,c);\n",
    "        pi = torch.from_numpy(pi).float().cuda()\n",
    "        #print pi[0,:]\n",
    "    #assert(type(pi)!=Nonetype)\n",
    "    loss = torch.sum(torch.mul(cost_matrix,pi))\n",
    "    return loss,pi\n",
    "\n",
    "def train(g_net):\n",
    "    data = dset.CIFAR10(root=\"/home/lrh/dataset/cifar-10\",train = False, download=True,transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "            ]))\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(data,batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "    g_optimizer = optim.Adam(g_net.parameters(),lr=lr,betas = beta)\n",
    "\n",
    "    for epoch in range(epoch_num):\n",
    "        for i,data in enumerate(dataloader,0):\n",
    "        #     #with shape [batch_size,3,32,32]\n",
    "             real_x = data[0]\n",
    "        #     z = torch.randn(real_x.size(0),100);\n",
    "        #     #to GPU\n",
    "        #     if cuda:\n",
    "        #         real_x = real_x.cuda()\n",
    "        #         z = z.cuda()\n",
    "        #\n",
    "        #     g_optimizer.zero_grad()\n",
    "        #\n",
    "        #     isopt = True\n",
    "        #     pi = None\n",
    "        #     for j in range(g_steps):\n",
    "        #         fake_x = g_net(z)\n",
    "        #         #[batch_size,3,32,32]\n",
    "        #         loss,pi = loss_fn(real_x,fake_x,isopt,pi)\n",
    "        #         loss.backward()\n",
    "        #         g_optimizer.step()\n",
    "        #         isopt = False\n",
    "        #\n",
    "        #\n",
    "        #     print \"epoch is:[{}|{}],index is:[{}|{}],g_loss:{}\".\\\n",
    "        #         format(epoch,epoch_num,\\\n",
    "        #         i,len(dataloader),loss);\n",
    "        # fake_x = g_net(z)\n",
    "        vutils.save_image(real_x.cpu().detach(),'%s/real_samples_epoch_%03d.png'\n",
    "        % (result_directory,epoch),normalize=True)\n",
    "        if epoch%50==0:\n",
    "            torch.save(g_net.state_dict(),'%s/gnet_%03d.pkl' %(result_directory,epoch));\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    g_net = wgan_generator()\n",
    "    g_net.apply(weights_init)\n",
    "    if cuda:\n",
    "        g_net.cuda()\n",
    "    #set moudle.istraing=True\n",
    "    #net.train()\n",
    "    train(g_net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# one_batch samples\n",
    "\n",
    "\n",
    "get a small numbers of samples\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**探究inception score的数学特性**\n",
    "\n",
    "batch_size:5000 \n",
    "\n",
    "9.85611644126555+0.503533570304311\n",
    "\n",
    "batch_size:1000\n",
    "\n",
    "8.291916482024153+0.43865066427286864\n",
    "\n",
    "batch_size:64\n",
    "\n",
    "3.4658015717169794+0.5964037687766206\n",
    "\n",
    "batch_size:100\n",
    "\n",
    "4.437662900437032+0.7272216898071227\n",
    "\n",
    "batch_size:200\n",
    "\n",
    "5.6565756770326, 0.645820972999001\n",
    "\n",
    "batch_size:400\n",
    "\n",
    "6.790523369170593, 0.7863464677535649\n",
    "\n",
    "batch_size:600\n",
    "\n",
    "7.517676544251043, 0.688784609754242\n",
    "\n",
    "$$inception\\_score = e^{E(\\sum_ip(y_i|x)log(\\frac{p(y_i|x)}{p(y_i)}))}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import torch.utils.data\n",
    "\n",
    "from torchvision.models.inception import inception_v3\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def inception_score(imgs, cuda=True, batch_size=32, resize=False, splits=1):\n",
    "    \"\"\"Computes the inception score of the generated images imgs\n",
    "    imgs -- Torch dataset of (3xHxW) numpy images normalized in the range [-1, 1]\n",
    "    cuda -- whether or not to run on GPU\n",
    "    batch_size -- batch size for feeding into Inception v3\n",
    "    splits -- number of splits\n",
    "    \"\"\"\n",
    "    N = len(imgs)\n",
    "\n",
    "    assert batch_size > 0\n",
    "    assert N > batch_size\n",
    "\n",
    "    # Set up dtype\n",
    "    if cuda:\n",
    "        dtype = torch.cuda.FloatTensor\n",
    "    else:\n",
    "        if torch.cuda.is_available():\n",
    "            print(\"WARNING: You have a CUDA device, so you should probably set cuda=True\")\n",
    "        dtype = torch.FloatTensor\n",
    "\n",
    "    # Set up dataloader\n",
    "    dataloader = torch.utils.data.DataLoader(imgs, batch_size=batch_size)\n",
    "\n",
    "    # Load inception model\n",
    "    inception_model = inception_v3(pretrained=True, transform_input=False).type(dtype)\n",
    "    inception_model.eval();\n",
    "    up = nn.Upsample(size=(299, 299), mode='bilinear').type(dtype)\n",
    "    def get_pred(x):\n",
    "        if resize:\n",
    "            x = up(x)\n",
    "        x = inception_model(x)\n",
    "        return F.softmax(x).data.cpu().numpy()\n",
    "\n",
    "    # Get predictions\n",
    "    preds = np.zeros((N, 1000))\n",
    "\n",
    "    for i, batch in enumerate(dataloader, 0):\n",
    "        batch = batch.type(dtype)\n",
    "        batchv = Variable(batch)\n",
    "        batch_size_i = batch.size()[0]\n",
    "\n",
    "        preds[i*batch_size:i*batch_size + batch_size_i] = get_pred(batchv)\n",
    "\n",
    "    # Now compute the mean kl-div\n",
    "    split_scores = []\n",
    "\n",
    "    for k in range(splits):\n",
    "        part = preds[k * (N // splits): (k+1) * (N // splits), :]\n",
    "        py = np.mean(part, axis=0)\n",
    "        scores = []\n",
    "        for i in range(part.shape[0]):\n",
    "            pyx = part[i, :]\n",
    "            scores.append(entropy(pyx, py))\n",
    "        split_scores.append(np.exp(np.mean(scores)))\n",
    "\n",
    "    return np.mean(split_scores), np.std(split_scores)\n",
    "\n",
    "\n",
    "class cifarDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self,images):\n",
    "            self.data = images\n",
    "        def __getitem__(self,index):\n",
    "            return self.data[index]\n",
    "        def __len__(self):\n",
    "            return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 32, 32])\n",
      "Calculating Inception Score...\n",
      "(3.6043417909946895, 0.5218633565924065)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lrh/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    class cifarDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self,images):\n",
    "            self.data = images\n",
    "        def __getitem__(self,index):\n",
    "            return self.data[index]\n",
    "        def __len__(self):\n",
    "            return len(self.data)\n",
    "\n",
    "    import torchvision.datasets as dset\n",
    "    import torchvision.transforms as transforms\n",
    "\n",
    "    cifar = dset.CIFAR10(root='/home/lrh/dataset/cifar-10', download=False,\n",
    "                             transform=transforms.Compose([\n",
    "                                 transforms.Scale(32),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                             ])\n",
    "    )\n",
    "    \n",
    "    dataloader = torch.utils.data.DataLoader(cifar,batch_size=64,shuffle=True,drop_last=True)\n",
    "    for i,data in enumerate(dataloader):\n",
    "        print data[0].shape\n",
    "        cifar = cifarDataset(data[0])\n",
    "        break;\n",
    "        \n",
    "    #IgnoreLabelDataset(cifar)\n",
    "    print (\"Calculating Inception Score...\")\n",
    "    print (inception_score(cifar, cuda=True, batch_size=32, resize=True, splits=10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算训练好的模型的inception score\n",
    "\n",
    "c:2_norm+0.05*1_norm\n",
    "\n",
    "IS-64:2.9020586246659446, 0.5355145676387948\n",
    "\n",
    "IS-100:3.575182050796198, 0.5209824971289462\n",
    "\n",
    "IS-200:4.38855363712337, 0.5608734183151969\n",
    "\n",
    "IS-400:4.8389852230529815, 0.560834277123877\n",
    "\n",
    "IS-600:5.5008282662264305, 0.389602242839442"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lrh/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5.5008282662264305, 0.389602242839442)\n",
      "(3.6043417909946895, 0.5218633565924065)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "nz = 100\n",
    "\n",
    "class wgan_generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(wgan_generator,self).__init__()\n",
    "        #input: [batch_size,100]\n",
    "        self.fc1 = nn.utils.weight_norm(nn.Linear(nz,2*4*4*1024))\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.utils.weight_norm(nn.Conv2d(1024,2*512,kernel_size=5,padding=2,stride=1))\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.utils.weight_norm(nn.Conv2d(512,2*256,kernel_size=5,padding=2,stride=1))\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.utils.weight_norm(nn.Conv2d(256,2*128,kernel_size=5,padding=2,stride=1))\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.utils.weight_norm(nn.Conv2d(128,3,kernel_size=5,padding=2,stride=1))\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x,l = torch.chunk(x,2,dim=1)\n",
    "        x = x * F.sigmoid(l) # gated linear unit, one of Alec's tricks\n",
    "\n",
    "        x = x.view(-1,1024,4,4)\n",
    "        #[4,4]\n",
    "        x = self.conv1(x)\n",
    "        x,l = torch.chunk(x,2,dim=1)\n",
    "        x = x * F.sigmoid(l)\n",
    "        #[8,8]\n",
    "        x = self.conv2(x)\n",
    "        x,l = torch.chunk(x,2,dim=1)\n",
    "        x = x * F.sigmoid(l)\n",
    "        #[16,16]\n",
    "        x = self.conv3(x)\n",
    "        x,l = torch.chunk(x,2,dim=1)\n",
    "        x = x * F.sigmoid(l)\n",
    "        #[32,32]\n",
    "        x = self.conv4(x)\n",
    "\n",
    "        x = F.tanh(x)\n",
    "        return x;\n",
    "    \n",
    "    \n",
    "load_gnet_directory = \"/home/lrh/program/git/pytorch-example/p-gan/result_smallcifar_1017_direct_all600data_onebatch_0.05LAMBDA/gnet_52000.pkl\"\n",
    "#load_gnet_directory = \"/home/lrh/program/git/pytorch-example/p-gan/result_smallcifar_1015_direct_64data_onebatch_0.05LAMBDA/gnet_60000.pkl\"\n",
    "g_net = wgan_generator()\n",
    "g_net.cuda()\n",
    "g_net.load_state_dict(torch.load(load_gnet_directory))\n",
    "\n",
    "z = torch.randn(600,nz);\n",
    "z = z.cuda()\n",
    "images = g_net(z)\n",
    "dataset = cifarDataset(images)\n",
    "print inception_score(dataset,resize=True,splits=10)\n",
    "print (inception_score(cifar, cuda=True, batch_size=32, resize=True, splits=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**真实图片与噪声的二阶范数**\n",
    "\n",
    "直接在生成图片的空间上做：\n",
    "\n",
    "1.探究高维数据n维范数失效的问题\n",
    "\n",
    "2.cos距离和其他的度量空间\n",
    "\n",
    "3.衡量图片的相似性指标\n",
    "\n",
    "    -ssim\n",
    "    \n",
    "    -msssim\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import utils\n",
    "import numpy as np\n",
    "batch_size = 64\n",
    "\n",
    "a1 = np.zeros([batch_size*batch_size,batch_size]);\n",
    "a2 = np.zeros([batch_size*batch_size,batch_size])\n",
    "for i in range(batch_size):\n",
    "    a1[batch_size*i:batch_size*(i+1),i] = 1\n",
    "    a2[batch_size*i:batch_size*(i+1),:] = np.eye(batch_size,batch_size)\n",
    "A = np.concatenate((a1,a2),axis=1)\n",
    "A = A.T\n",
    "b = np.ones([batch_size*2])/batch_size\n",
    "\n",
    "def loss_fn(real_x,fake_x,isopt=True,pi=None):\n",
    "    batch_size = real_x.size(0)\n",
    "    real_x = real_x.view(batch_size,-1)\n",
    "    fake_x = fake_x.view(batch_size,-1)\n",
    "    \n",
    "    cost_matrix = torch.zeros([batch_size,batch_size]).cuda()\n",
    "    dist_matrix = np.zeros([batch_size,batch_size])\n",
    "    \n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        diff = real_x - fake_x[i]\n",
    "        #||x_i - y_j||_2\n",
    "        #[batch_size,1]\n",
    "        diff_norm = diff.norm(2 , dim = 1)\n",
    "        #||x_i - y_j||_1\n",
    "        diff_norm_1 = diff.norm(1 , dim = 1)\n",
    "        #cost_matrix[:,i] = diff_norm\n",
    "        #+ 0.05 * diff_norm_1\n",
    "        #min_value,min_pos = torch.min(diff_norm,dim=0)\n",
    "        #print min_pos\n",
    "        #dist_matrix[min_pos,i] = 1.0/batch_size\n",
    "        #cost_matrix[i,i] = 9999\n",
    "    #[batch_size,batch_size]\n",
    "    #cosin distance\n",
    "    cos_diff = cosin_loss(real_x,fake_x)\n",
    "    cost_matrix = cos_diff\n",
    "    \n",
    "    if isopt:\n",
    "        c = cost_matrix.cpu().detach().numpy().reshape(batch_size*batch_size)\n",
    "        pi = utils.simplexCVX(batch_size,c);\n",
    "        pi = torch.from_numpy(pi).float().cuda()\n",
    "        #print pi[0,:]\n",
    "    #assert(type(pi)!=Nonetype)\n",
    "    loss = torch.sum(torch.mul(cost_matrix,pi))\n",
    "    return loss,pi\n",
    "\n",
    "def cosin_loss(real_x,fake_x):\n",
    "    #cosin distance\n",
    "    norm_real_x = real_x / real_x.norm(2,dim=1,keepdim=True)\n",
    "    norm_fake_x = fake_x / fake_x.norm(2,dim=1,keepdim=True)\n",
    "    \n",
    "    #shape [batch_size,batch_size]\n",
    "    inner_product = torch.matmul(norm_real_x,norm_fake_x.transpose(1,0))\n",
    "    loss = 1 - inner_product\n",
    "    return loss\n",
    "\n",
    "data = dset.CIFAR10(root=\"/home/lrh/dataset/cifar-10\",train = False, download=True,transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "            ]))\n",
    "\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(data,batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "\n",
    "for i,data in enumerate(dataloader):\n",
    "    if i==0:\n",
    "        img1 = data[0].cuda()\n",
    "    elif i==1:\n",
    "        img2 = data[0].cuda()\n",
    "    else:\n",
    "        break\n",
    "\n",
    "print loss_fn(img1,img2)\n",
    "\n",
    "g = (torch.randn(batch_size,3,32,32)-0.5).cuda()\n",
    "    \n",
    "\n",
    "print loss_fn(img2,g)  \n",
    "\n",
    "\n",
    "img1 = img1.view(batch_size,-1)\n",
    "img2 = img2.view(batch_size,-1)\n",
    "g = g.view(batch_size,-1)\n",
    "\n",
    "#print (img1-img2).norm(2,dim=1).mean()\n",
    "#print (img1-g).norm(2,dim=1).mean()\n",
    "\n",
    "#print cosin_loss(img1,img2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cifar生成\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dset\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "from scipy.optimize import linprog\n",
    "import torch.nn.functional as F\n",
    "import utils\n",
    "\n",
    "nz = 100\n",
    "g_steps = 5\n",
    "result_directory = \"result_cifar_1009_direct_onebatch16_0.05LAMBDA\"\n",
    "batch_size = 16\n",
    "bn = True\n",
    "lr = 3e-4\n",
    "beta = (0.5,0.999)\n",
    "epoch_num = 1000\n",
    "cuda = True\n",
    "\n",
    "\n",
    "a1 = np.zeros([batch_size*batch_size,batch_size]);\n",
    "a2 = np.zeros([batch_size*batch_size,batch_size])\n",
    "for i in range(batch_size):\n",
    "    a1[batch_size*i:batch_size*(i+1),i] = 1\n",
    "    a2[batch_size*i:batch_size*(i+1),:] = np.eye(batch_size,batch_size)\n",
    "A = np.concatenate((a1,a2),axis=1)\n",
    "A = A.T\n",
    "b = np.ones([batch_size*2])/batch_size\n",
    "\n",
    "\n",
    "class wgan_generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(wgan_generator,self).__init__()\n",
    "        #input: [batch_size,100]\n",
    "        self.fc1 = nn.utils.weight_norm(nn.Linear(nz,2*4*4*1024))\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.utils.weight_norm(nn.Conv2d(1024,2*512,kernel_size=5,padding=2,stride=1))\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.utils.weight_norm(nn.Conv2d(512,2*256,kernel_size=5,padding=2,stride=1))\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.utils.weight_norm(nn.Conv2d(256,2*128,kernel_size=5,padding=2,stride=1))\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.utils.weight_norm(nn.Conv2d(128,3,kernel_size=5,padding=2,stride=1))\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x,l = torch.chunk(x,2,dim=1)\n",
    "        x = x * F.sigmoid(l) # gated linear unit, one of Alec's tricks\n",
    "        x = x.view(-1,1024,4,4)\n",
    "        #[4,4]\n",
    "        x = self.conv1(x)\n",
    "        x,l = torch.chunk(x,2,dim=1)\n",
    "        x = x * F.sigmoid(l)\n",
    "        #[8,8]\n",
    "        x = self.conv2(x)\n",
    "        x,l = torch.chunk(x,2,dim=1)\n",
    "        x = x * F.sigmoid(l)\n",
    "        #[16,16]\n",
    "        x = self.conv3(x)\n",
    "        x,l = torch.chunk(x,2,dim=1)\n",
    "        x = x * F.sigmoid(l)\n",
    "        #[32,32]\n",
    "        x = self.conv4(x)\n",
    "\n",
    "        x = F.tanh(x)\n",
    "        return x;\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "def loss_fn(real_x,fake_x,isopt=True,pi=None):\n",
    "    batch_size = real_x.size(0)\n",
    "    cost_matrix = torch.zeros([batch_size,batch_size]).cuda()\n",
    "    dist_matrix = np.zeros([batch_size,batch_size])\n",
    "    for i in range(batch_size):\n",
    "        diff = real_x - fake_x[i]\n",
    "        diff = diff.view(diff.size(0),-1)\n",
    "        #||x_i - y_j||_2\n",
    "        #[batch_size,1]\n",
    "        diff_norm = diff.norm(2 , dim = 1)\n",
    "        diff_norm_1 = diff.norm(1 , dim = 1)\n",
    "        cost_matrix[:,i] = diff_norm + 0.1 * diff_norm_1\n",
    "        #min_value,min_pos = torch.min(diff_norm,dim=0)\n",
    "        #print min_pos\n",
    "        #dist_matrix[min_pos,i] = 1.0/batch_size\n",
    "    #[batch_size,batch_size]\n",
    "    if isopt:\n",
    "        c = cost_matrix.cpu().detach().numpy().reshape(batch_size*batch_size)\n",
    "        pi = utils.simplexCVX(batch_size,c);\n",
    "        pi = torch.from_numpy(pi).float().cuda()\n",
    "        #print pi[0,:]\n",
    "    #assert(type(pi)!=Nonetype)\n",
    "    loss = torch.sum(torch.mul(cost_matrix,pi))\n",
    "    return loss,pi\n",
    "\n",
    "def train(g_net):\n",
    "    data = dset.CIFAR10(root=\"/home/lrh/dataset/cifar-10\",train = False, download=True,transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "            ]))\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(data,batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "    g_optimizer = optim.Adam(g_net.parameters(),lr=lr,betas = beta)\n",
    "\n",
    "    for i,data in enumerate(dataloader,0):\n",
    "        #     #with shape [batch_size,3,32,32]\n",
    "        real_x = data[0]\n",
    "    for epoch in range(epoch_num):\n",
    "        z = torch.randn(real_x.size(0),nz);\n",
    "        #to GPU\n",
    "        if cuda:\n",
    "            real_x = real_x.cuda()\n",
    "            z = z.cuda()\n",
    "\n",
    "        g_optimizer.zero_grad()\n",
    "        isopt = True\n",
    "        pi = None\n",
    "        for j in range(g_steps):\n",
    "            fake_x = g_net(z)\n",
    "            #[batch_size,3,32,32]\n",
    "            loss,pi = loss_fn(real_x,fake_x,isopt,pi)\n",
    "            loss.backward()\n",
    "            g_optimizer.step()\n",
    "            isopt = False\n",
    "\n",
    "\n",
    "        fake_x = g_net(z)\n",
    "        vutils.save_image(fake_x.cpu().detach(),'%s/real_samples_epoch_%03d.png'\n",
    "        % (result_directory,epoch),normalize=True)\n",
    "        if epoch%50==0:\n",
    "            torch.save(g_net.state_dict(),'%s/gnet_%03d.pkl' %(result_directory,epoch));\n",
    "            print \"epoch is:[{}|{}],index is:[{}|{}],g_loss:{}\".\\\n",
    "                format(epoch,epoch_num,\\\n",
    "                i,len(dataloader),loss);\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    g_net = wgan_generator()\n",
    "    g_net.apply(weights_init)\n",
    "    if cuda:\n",
    "        g_net.cuda()\n",
    "    #set moudle.istraing=True\n",
    "    #net.train()\n",
    "    train(g_net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lsun生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dset\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "from scipy.optimize import linprog\n",
    "import torch.nn.functional as F\n",
    "import utils\n",
    "\n",
    "nz = 100\n",
    "g_steps = 5\n",
    "result_directory = \"result_lsun_1011_direct_onebatch\"\n",
    "batch_size = 16\n",
    "bn = True\n",
    "lr = 3e-4\n",
    "beta = (0.5,0.999)\n",
    "epoch_num = 1000\n",
    "cuda = True\n",
    "\n",
    "\n",
    "a1 = np.zeros([batch_size*batch_size,batch_size]);\n",
    "a2 = np.zeros([batch_size*batch_size,batch_size])\n",
    "for i in range(batch_size):\n",
    "    a1[batch_size*i:batch_size*(i+1),i] = 1\n",
    "    a2[batch_size*i:batch_size*(i+1),:] = np.eye(batch_size,batch_size)\n",
    "A = np.concatenate((a1,a2),axis=1)\n",
    "A = A.T\n",
    "b = np.ones([batch_size*2])/batch_size\n",
    "\n",
    "\n",
    "class wgan_generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(wgan_generator,self).__init__()\n",
    "        #input: [batch_size,100]\n",
    "        self.fc1 = nn.utils.weight_norm(nn.Linear(nz,2*4*4*1024))\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.utils.weight_norm(nn.Conv2d(1024,2*512,kernel_size=5,padding=2,stride=1))\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.utils.weight_norm(nn.Conv2d(512,2*256,kernel_size=5,padding=2,stride=1))\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.utils.weight_norm(nn.Conv2d(256,2*128,kernel_size=5,padding=2,stride=1))\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.utils.weight_norm(nn.Conv2d(128,3,kernel_size=5,padding=2,stride=1))\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x,l = torch.chunk(x,2,dim=1)\n",
    "        x = x * F.sigmoid(l) # gated linear unit, one of Alec's tricks\n",
    "        x = x.view(-1,1024,4,4)\n",
    "        #[4,4]\n",
    "        x = self.conv1(x)\n",
    "        x,l = torch.chunk(x,2,dim=1)\n",
    "        x = x * F.sigmoid(l)\n",
    "        #[8,8]\n",
    "        x = self.conv2(x)\n",
    "        x,l = torch.chunk(x,2,dim=1)\n",
    "        x = x * F.sigmoid(l)\n",
    "        #[16,16]\n",
    "        x = self.conv3(x)\n",
    "        x,l = torch.chunk(x,2,dim=1)\n",
    "        x = x * F.sigmoid(l)\n",
    "        #[32,32]\n",
    "        x = self.conv4(x)\n",
    "\n",
    "        x = F.tanh(x)\n",
    "        return x;\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "def loss_fn(real_x,fake_x,isopt=True,pi=None):\n",
    "    batch_size = real_x.size(0)\n",
    "    cost_matrix = torch.zeros([batch_size,batch_size]).cuda()\n",
    "    dist_matrix = np.zeros([batch_size,batch_size])\n",
    "    for i in range(batch_size):\n",
    "        diff = real_x - fake_x[i]\n",
    "        diff = diff.view(diff.size(0),-1)\n",
    "        #||x_i - y_j||_2\n",
    "        #[batch_size,1]\n",
    "        diff_norm = diff.norm(2 , dim = 1)\n",
    "        diff_norm_1 = diff.norm(1 , dim = 1)\n",
    "        cost_matrix[:,i] = diff_norm + 0.1 * diff_norm_1\n",
    "        #min_value,min_pos = torch.min(diff_norm,dim=0)\n",
    "        #print min_pos\n",
    "        #dist_matrix[min_pos,i] = 1.0/batch_size\n",
    "    #[batch_size,batch_size]\n",
    "    if isopt:\n",
    "        c = cost_matrix.cpu().detach().numpy().reshape(batch_size*batch_size)\n",
    "        pi = utils.simplexCVX(batch_size,c);\n",
    "        pi = torch.from_numpy(pi).float().cuda()\n",
    "        #print pi[0,:]\n",
    "    #assert(type(pi)!=Nonetype)\n",
    "    loss = torch.sum(torch.mul(cost_matrix,pi))\n",
    "    return loss,pi\n",
    "\n",
    "def train(g_net):\n",
    "    data = dset.CIFAR10(root=\"/home/lrh/dataset/cifar-10\",train = False, download=True,transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "            ]))\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(data,batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "    g_optimizer = optim.Adam(g_net.parameters(),lr=lr,betas = beta)\n",
    "\n",
    "    for i,data in enumerate(dataloader,0):\n",
    "        #     #with shape [batch_size,3,32,32]\n",
    "        real_x = data[0]\n",
    "    for epoch in range(epoch_num):\n",
    "        z = torch.randn(real_x.size(0),nz);\n",
    "        #to GPU\n",
    "        if cuda:\n",
    "            real_x = real_x.cuda()\n",
    "            z = z.cuda()\n",
    "\n",
    "        g_optimizer.zero_grad()\n",
    "        isopt = True\n",
    "        pi = None\n",
    "        for j in range(g_steps):\n",
    "            fake_x = g_net(z)\n",
    "            #[batch_size,3,32,32]\n",
    "            loss,pi = loss_fn(real_x,fake_x,isopt,pi)\n",
    "            loss.backward()\n",
    "            g_optimizer.step()\n",
    "            isopt = False\n",
    "\n",
    "\n",
    "        fake_x = g_net(z)\n",
    "        vutils.save_image(fake_x.cpu().detach(),'%s/real_samples_epoch_%03d.png'\n",
    "        % (result_directory,epoch),normalize=True)\n",
    "        if epoch%50==0:\n",
    "            torch.save(g_net.state_dict(),'%s/gnet_%03d.pkl' %(result_directory,epoch));\n",
    "            print \"epoch is:[{}|{}],index is:[{}|{}],g_loss:{}\".\\\n",
    "                format(epoch,epoch_num,\\\n",
    "                i,len(dataloader),loss);\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    g_net = wgan_generator()\n",
    "    g_net.apply(weights_init)\n",
    "    if cuda:\n",
    "        g_net.cuda()\n",
    "    #set moudle.istraing=True\n",
    "    #net.train()\n",
    "    train(g_net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mnist 生成\n",
    "\n",
    "结论1：\n",
    "\n",
    "该模型可以学习到少量样本的分布，\n",
    "\n",
    "比如只取一个batch用作训练样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dset\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "from scipy.optimize import linprog\n",
    "import torch.nn.functional as F\n",
    "import utils\n",
    "\n",
    "nz = 100\n",
    "g_steps = 5\n",
    "result_directory = \"result_cifar_1009_direct_0.05LAMBDA\"\n",
    "batch_size = 64\n",
    "bn = True\n",
    "lr = 3e-4\n",
    "beta = (0.5,0.999)\n",
    "epoch_num = 1000\n",
    "cuda = True\n",
    "\n",
    "\n",
    "a1 = np.zeros([batch_size*batch_size,batch_size]);\n",
    "a2 = np.zeros([batch_size*batch_size,batch_size])\n",
    "for i in range(batch_size):\n",
    "    a1[batch_size*i:batch_size*(i+1),i] = 1\n",
    "    a2[batch_size*i:batch_size*(i+1),:] = np.eye(batch_size,batch_size)\n",
    "A = np.concatenate((a1,a2),axis=1)\n",
    "A = A.T\n",
    "b = np.ones([batch_size*2])/batch_size\n",
    "\n",
    "\n",
    "class wgan_generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(wgan_generator,self).__init__()\n",
    "        #input: [batch_size,100]\n",
    "        self.fc1 = nn.utils.weight_norm(nn.Linear(nz,2*4*4*1024))\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.utils.weight_norm(nn.Conv2d(1024,2*512,kernel_size=5,padding=2,stride=1))\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.utils.weight_norm(nn.Conv2d(512,2*256,kernel_size=5,padding=1,stride=1))\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.utils.weight_norm(nn.Conv2d(256,2*128,kernel_size=5,padding=2,stride=1))\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.utils.weight_norm(nn.Conv2d(128,1,kernel_size=5,padding=2,stride=1))\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x,l = torch.chunk(x,2,dim=1)\n",
    "        x = x * F.sigmoid(l) # gated linear unit, one of Alec's tricks\n",
    "\n",
    "        x = x.view(-1,1024,4,4)\n",
    "        #[4,4]\n",
    "        x = self.conv1(x)\n",
    "        x,l = torch.chunk(x,2,dim=1)\n",
    "        x = x * F.sigmoid(l)\n",
    "        #[8,8]\n",
    "        x = self.conv2(x)\n",
    "        x,l = torch.chunk(x,2,dim=1)\n",
    "        x = x * F.sigmoid(l)\n",
    "        #[14,14]\n",
    "        x = self.conv3(x)\n",
    "        x,l = torch.chunk(x,2,dim=1)\n",
    "        x = x * F.sigmoid(l)\n",
    "        #[28,28]\n",
    "        x = self.conv4(x)\n",
    "\n",
    "        x = F.tanh(x)\n",
    "        return x;\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "def loss_fn(real_x,fake_x,isopt=True,pi=None):\n",
    "    batch_size = real_x.size(0)\n",
    "    cost_matrix = torch.zeros([batch_size,batch_size]).cuda()\n",
    "    dist_matrix = np.zeros([batch_size,batch_size])\n",
    "    for i in range(batch_size):\n",
    "        diff = real_x - fake_x[i]\n",
    "        diff = diff.view(diff.size(0),-1)\n",
    "        #||x_i - y_j||_2\n",
    "        #[batch_size,1]\n",
    "        diff_norm = diff.norm(2 , dim = 1)\n",
    "        diff_norm_1 = diff.norm(1 , dim = 1)\n",
    "        cost_matrix[:,i] = diff_norm + 0.05 * diff_norm_1\n",
    "        #min_value,min_pos = torch.min(diff_norm,dim=0)\n",
    "        #print min_pos\n",
    "        #dist_matrix[min_pos,i] = 1.0/batch_size\n",
    "    #[batch_size,batch_size]\n",
    "    if isopt:\n",
    "        c = cost_matrix.cpu().detach().numpy().reshape(batch_size*batch_size)\n",
    "        pi = utils.simplexCVX(batch_size,c);\n",
    "        pi = torch.from_numpy(pi).float().cuda()\n",
    "        #print pi[0,:]\n",
    "    #assert(type(pi)!=Nonetype)\n",
    "    loss = torch.sum(torch.mul(cost_matrix,pi))\n",
    "    return loss,pi\n",
    "\n",
    "def train(g_net):\n",
    "    data = dset.MNIST(root=\"/home/lrh/dataset/mnist\",train = False, download=False,transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "            ]))\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(data,batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "    g_optimizer = optim.Adam(g_net.parameters(),lr=lr,betas = beta)\n",
    "\n",
    "    for i,data in enumerate(dataloader,0):\n",
    "        #     #with shape [batch_size,3,32,32]\n",
    "        real_x = data[0]\n",
    "    for epoch in range(epoch_num):\n",
    "        z = torch.randn(real_x.size(0),nz);\n",
    "        #to GPU\n",
    "        if cuda:\n",
    "            real_x = real_x.cuda()\n",
    "            z = z.cuda()\n",
    "\n",
    "        g_optimizer.zero_grad()\n",
    "    #\n",
    "        isopt = True\n",
    "        pi = None\n",
    "        for j in range(g_steps):\n",
    "            fake_x = g_net(z)\n",
    "            #[batch_size,3,32,32]\n",
    "            loss,pi = loss_fn(real_x,fake_x,isopt,pi)\n",
    "            loss.backward()\n",
    "            g_optimizer.step()\n",
    "            isopt = False\n",
    "            \n",
    "        fake_x = g_net(z)\n",
    "        vutils.save_image(fake_x.cpu().detach(),'%s/real_samples_epoch_%03d.png'\n",
    "        % (result_directory,epoch),normalize=True)\n",
    "        if epoch%50==0:\n",
    "            torch.save(g_net.state_dict(),'%s/gnet_%03d.pkl' %(result_directory,epoch));\n",
    "            print \"epoch is:[{}|{}],index is:[{}|{}],g_loss:{}\".\\\n",
    "                format(epoch,epoch_num,\\\n",
    "                i,len(dataloader),loss);\n",
    "\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    g_net = wgan_generator()\n",
    "    g_net.apply(weights_init)\n",
    "    if cuda:\n",
    "        g_net.cuda()\n",
    "    #set moudle.istraing=True\n",
    "    #net.train()\n",
    "    train(g_net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mnist \n",
    "\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dset\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "from scipy.optimize import linprog\n",
    "import torch.nn.functional as F\n",
    "import utils\n",
    "\n",
    "nz = 100\n",
    "g_steps = 5\n",
    "result_directory = \"result_mnist_1011_test_data_0.05LAMBDA\"\n",
    "batch_size = 64\n",
    "bn = True\n",
    "lr = 3e-4\n",
    "beta = (0.5,0.999)\n",
    "epoch_num = 1000\n",
    "cuda = True\n",
    "\n",
    "\n",
    "a1 = np.zeros([batch_size*batch_size,batch_size]);\n",
    "a2 = np.zeros([batch_size*batch_size,batch_size])\n",
    "for i in range(batch_size):\n",
    "    a1[batch_size*i:batch_size*(i+1),i] = 1\n",
    "    a2[batch_size*i:batch_size*(i+1),:] = np.eye(batch_size,batch_size)\n",
    "A = np.concatenate((a1,a2),axis=1)\n",
    "A = A.T\n",
    "b = np.ones([batch_size*2])/batch_size\n",
    "\n",
    "\n",
    "class wgan_generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(wgan_generator,self).__init__()\n",
    "        #input: [batch_size,100]\n",
    "        self.fc1 = nn.utils.weight_norm(nn.Linear(nz,2*4*4*1024))\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.utils.weight_norm(nn.Conv2d(1024,2*512,kernel_size=5,padding=2,stride=1))\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.utils.weight_norm(nn.Conv2d(512,2*256,kernel_size=5,padding=1,stride=1))\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.utils.weight_norm(nn.Conv2d(256,2*128,kernel_size=5,padding=2,stride=1))\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.utils.weight_norm(nn.Conv2d(128,1,kernel_size=5,padding=2,stride=1))\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x,l = torch.chunk(x,2,dim=1)\n",
    "        x = x * F.sigmoid(l) # gated linear unit, one of Alec's tricks\n",
    "\n",
    "        x = x.view(-1,1024,4,4)\n",
    "        #[4,4]\n",
    "        x = self.conv1(x)\n",
    "        x,l = torch.chunk(x,2,dim=1)\n",
    "        x = x * F.sigmoid(l)\n",
    "        #[8,8]\n",
    "        x = self.conv2(x)\n",
    "        x,l = torch.chunk(x,2,dim=1)\n",
    "        x = x * F.sigmoid(l)\n",
    "        #[14,14]\n",
    "        x = self.conv3(x)\n",
    "        x,l = torch.chunk(x,2,dim=1)\n",
    "        x = x * F.sigmoid(l)\n",
    "        #[28,28]\n",
    "        x = self.conv4(x)\n",
    "\n",
    "        x = F.tanh(x)\n",
    "        return x;\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "def loss_fn(real_x,fake_x,isopt=True,pi=None):\n",
    "    batch_size = real_x.size(0)\n",
    "    cost_matrix = torch.zeros([batch_size,batch_size]).cuda()\n",
    "    dist_matrix = np.zeros([batch_size,batch_size])\n",
    "    for i in range(batch_size):\n",
    "        diff = real_x - fake_x[i]\n",
    "        diff = diff.view(diff.size(0),-1)\n",
    "        #||x_i - y_j||_2\n",
    "        #[batch_size,1]\n",
    "        diff_norm = diff.norm(2 , dim = 1)\n",
    "        diff_norm_1 = diff.norm(1 , dim = 1)\n",
    "        cost_matrix[:,i] = diff_norm + 0.05 * diff_norm_1\n",
    "        #min_value,min_pos = torch.min(diff_norm,dim=0)\n",
    "        #print min_pos\n",
    "        #dist_matrix[min_pos,i] = 1.0/batch_size\n",
    "    #[batch_size,batch_size]\n",
    "    if isopt:\n",
    "        c = cost_matrix.cpu().detach().numpy().reshape(batch_size*batch_size)\n",
    "        pi = utils.simplexCVX(batch_size,c);\n",
    "        pi = torch.from_numpy(pi).float().cuda()\n",
    "        #print pi[0,:]\n",
    "    #assert(type(pi)!=Nonetype)\n",
    "    loss = torch.sum(torch.mul(cost_matrix,pi))\n",
    "    return loss,pi\n",
    "\n",
    "def train(g_net):\n",
    "    data = dset.MNIST(root=\"/home/lrh/dataset/mnist\",train = False, download=False,transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "            ]))\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(data,batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "    g_optimizer = optim.Adam(g_net.parameters(),lr=lr,betas = beta)\n",
    "\n",
    "    \n",
    "    for epoch in range(epoch_num):\n",
    "        for i,data in enumerate(dataloader,0):\n",
    "        #with shape [batch_size,3,32,32]\n",
    "            real_x = data[0]\n",
    "            z = torch.randn(real_x.size(0),nz);\n",
    "            #to GPU\n",
    "            if cuda:\n",
    "                real_x = real_x.cuda()\n",
    "                z = z.cuda()\n",
    "\n",
    "            g_optimizer.zero_grad()\n",
    "        #\n",
    "            isopt = True\n",
    "            pi = None\n",
    "            for j in range(g_steps):\n",
    "                fake_x = g_net(z)\n",
    "                #[batch_size,3,32,32]\n",
    "                loss,pi = loss_fn(real_x,fake_x,isopt,pi)\n",
    "                loss.backward()\n",
    "                g_optimizer.step()\n",
    "                isopt = False\n",
    "\n",
    "        fake_x = g_net(z)\n",
    "        vutils.save_image(fake_x.cpu().detach(),'%s/real_samples_epoch_%03d.png'\n",
    "        % (result_directory,epoch),normalize=True)\n",
    "        if epoch%50==0:\n",
    "            torch.save(g_net.state_dict(),'%s/gnet_%03d.pkl' %(result_directory,epoch));\n",
    "            print \"epoch is:[{}|{}],index is:[{}|{}],g_loss:{}\".\\\n",
    "                format(epoch,epoch_num,\\\n",
    "                i,len(dataloader),loss);\n",
    "\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    g_net = wgan_generator()\n",
    "    g_net.apply(weights_init)\n",
    "    if cuda:\n",
    "        g_net.cuda()\n",
    "    #set moudle.istraing=True\n",
    "    #net.train()\n",
    "    train(g_net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.Tensor([[1,2,4,5]])\n",
    "print a.transpose(1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在特征空间匹配\n",
    "\n",
    "1.autoencoder\n",
    "\n",
    "2.inception-v3 classify"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
